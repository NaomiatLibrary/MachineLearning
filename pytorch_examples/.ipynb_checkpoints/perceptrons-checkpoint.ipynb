{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全く同じネットワークを4つの書き方で動かします\n",
    "https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "\n",
    "日本語:https://www.aiprogrammers.net/entry/2020/04/24/092500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## warmup:Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 38520443.548501045\n",
      "1 40106890.92393669\n",
      "2 46893996.87852523\n",
      "3 47746784.01847948\n",
      "4 36180681.05554907\n",
      "5 19098472.562031403\n",
      "6 8047113.291170775\n",
      "7 3575785.7939848537\n",
      "8 2058388.3799723121\n",
      "9 1475382.4084175427\n",
      "10 1176879.5750739207\n",
      "11 979065.1062340576\n",
      "12 829443.3354382712\n",
      "13 709724.5548966997\n",
      "14 611402.3335875792\n",
      "15 529567.3011789686\n",
      "16 460888.8999882756\n",
      "17 402883.3631774767\n",
      "18 353526.20843780256\n",
      "19 311317.75013024977\n",
      "20 275091.791918296\n",
      "21 243878.65363627818\n",
      "22 216855.47681941916\n",
      "23 193389.87806939502\n",
      "24 172949.28498883572\n",
      "25 155044.60639267668\n",
      "26 139373.54823776308\n",
      "27 125546.38455738673\n",
      "28 113316.25164179596\n",
      "29 102466.45856108962\n",
      "30 92819.01325240743\n",
      "31 84217.38329398693\n",
      "32 76535.2176056859\n",
      "33 69658.33989214539\n",
      "34 63486.7426855221\n",
      "35 57942.93237944448\n",
      "36 52948.43307965083\n",
      "37 48445.87693746871\n",
      "38 44376.91893667788\n",
      "39 40693.78646329875\n",
      "40 37354.19900520337\n",
      "41 34322.569454176424\n",
      "42 31566.01053127266\n",
      "43 29058.269215715\n",
      "44 26772.385622017355\n",
      "45 24684.33600940508\n",
      "46 22777.092089598293\n",
      "47 21032.882438057713\n",
      "48 19438.80024082609\n",
      "49 17978.19779482216\n",
      "50 16637.704344626007\n",
      "51 15406.856631238257\n",
      "52 14276.330323283604\n",
      "53 13240.269764421806\n",
      "54 12286.997404362668\n",
      "55 11408.533064554164\n",
      "56 10598.403616368645\n",
      "57 9851.300448475995\n",
      "58 9161.349451255777\n",
      "59 8523.695618470838\n",
      "60 7934.506681599187\n",
      "61 7390.118597633853\n",
      "62 6886.071936364135\n",
      "63 6419.132941041165\n",
      "64 5986.257534779679\n",
      "65 5584.501678978205\n",
      "66 5211.625337172165\n",
      "67 4865.55367521848\n",
      "68 4544.070585709496\n",
      "69 4245.102921310253\n",
      "70 3967.088192590647\n",
      "71 3708.4844029066444\n",
      "72 3467.8907769263787\n",
      "73 3243.8677714457253\n",
      "74 3035.1969051763967\n",
      "75 2840.7808585086887\n",
      "76 2659.5761923704995\n",
      "77 2490.508314939004\n",
      "78 2332.881888803568\n",
      "79 2185.740056843687\n",
      "80 2048.3366675634907\n",
      "81 1920.0656965635187\n",
      "82 1800.2557553576753\n",
      "83 1688.2946642278562\n",
      "84 1583.6612821632434\n",
      "85 1485.8173133357195\n",
      "86 1394.2927208371034\n",
      "87 1308.6967270972782\n",
      "88 1228.603471338421\n",
      "89 1153.6245376475515\n",
      "90 1083.4407091937433\n",
      "91 1017.726527544523\n",
      "92 956.1601333036443\n",
      "93 898.4691302967176\n",
      "94 844.4076332211876\n",
      "95 793.7388749061583\n",
      "96 746.2339791987513\n",
      "97 701.7127697387995\n",
      "98 659.9471354023572\n",
      "99 620.7494006196491\n",
      "100 583.9718344639957\n",
      "101 549.4660998770279\n",
      "102 517.0818699562924\n",
      "103 486.67605588130215\n",
      "104 458.13246781041374\n",
      "105 431.3226130040897\n",
      "106 406.1324560683181\n",
      "107 382.46409971135665\n",
      "108 360.22759299870387\n",
      "109 339.3275433852773\n",
      "110 319.6853506884545\n",
      "111 301.2227462387551\n",
      "112 283.85441579111756\n",
      "113 267.52293125236156\n",
      "114 252.16305214537326\n",
      "115 237.7127986357951\n",
      "116 224.11781560706962\n",
      "117 211.32679125759213\n",
      "118 199.28819699354722\n",
      "119 187.95954981532446\n",
      "120 177.29192191851362\n",
      "121 167.24881987248543\n",
      "122 157.79347862576952\n",
      "123 148.8906752488656\n",
      "124 140.50225837056675\n",
      "125 132.599633785723\n",
      "126 125.15485928113287\n",
      "127 118.14211188976823\n",
      "128 111.53324456301833\n",
      "129 105.3046774033609\n",
      "130 99.43354992157614\n",
      "131 93.90029515715725\n",
      "132 88.6848519648987\n",
      "133 83.76720167479206\n",
      "134 79.13147406107439\n",
      "135 74.75843873425231\n",
      "136 70.63354155994787\n",
      "137 66.74163471915257\n",
      "138 63.06998922791125\n",
      "139 59.60745373155435\n",
      "140 56.33946971722246\n",
      "141 53.25447120654454\n",
      "142 50.343362462702004\n",
      "143 47.595896952340794\n",
      "144 45.00260731442829\n",
      "145 42.5541329951623\n",
      "146 40.24225031847207\n",
      "147 38.055043342064565\n",
      "148 35.98984580403102\n",
      "149 34.040512969911305\n",
      "150 32.1986207588135\n",
      "151 30.459531127212067\n",
      "152 28.816393688115134\n",
      "153 27.263811013501552\n",
      "154 25.797881849611414\n",
      "155 24.412888823094818\n",
      "156 23.10347011575456\n",
      "157 21.866314894074605\n",
      "158 20.69728200110382\n",
      "159 19.59235076147197\n",
      "160 18.547664474176624\n",
      "161 17.560186423511936\n",
      "162 16.62658457189261\n",
      "163 15.743880830721\n",
      "164 14.909190657112124\n",
      "165 14.119695850546401\n",
      "166 13.373166633757277\n",
      "167 12.667146806622362\n",
      "168 11.999210004272328\n",
      "169 11.367194441775506\n",
      "170 10.769584119036011\n",
      "171 10.203943580099885\n",
      "172 9.668776434018362\n",
      "173 9.16256120517015\n",
      "174 8.683285401405465\n",
      "175 8.229652984274106\n",
      "176 7.800415729848331\n",
      "177 7.394120370031771\n",
      "178 7.009475922969976\n",
      "179 6.645307120830151\n",
      "180 6.300425738731502\n",
      "181 5.973991339151667\n",
      "182 5.66491525448341\n",
      "183 5.372108133258212\n",
      "184 5.09489206489101\n",
      "185 4.83229169789328\n",
      "186 4.583558184030716\n",
      "187 4.34795439861122\n",
      "188 4.124782013214546\n",
      "189 3.9132788431399046\n",
      "190 3.7129170770102427\n",
      "191 3.523050064932588\n",
      "192 3.343148915296315\n",
      "193 3.172618318833406\n",
      "194 3.0110080946198226\n",
      "195 2.8579353996763515\n",
      "196 2.7127394978392276\n",
      "197 2.5750970396875514\n",
      "198 2.4446215367637754\n",
      "199 2.320916054306287\n",
      "200 2.2036364395120804\n",
      "201 2.0923998818983485\n",
      "202 1.9869259875656744\n",
      "203 1.8869328153857523\n",
      "204 1.7920541826423322\n",
      "205 1.7020631932217019\n",
      "206 1.6167387401594882\n",
      "207 1.535765135200708\n",
      "208 1.4589566296411376\n",
      "209 1.3860943715264478\n",
      "210 1.3169456602845093\n",
      "211 1.2513358814735365\n",
      "212 1.1890755796925616\n",
      "213 1.1299902530644115\n",
      "214 1.073922700342352\n",
      "215 1.020702285832975\n",
      "216 0.9701780805630126\n",
      "217 0.9222199479059161\n",
      "218 0.8766966623948673\n",
      "219 0.8334733445685978\n",
      "220 0.792436930334745\n",
      "221 0.7534663560990701\n",
      "222 0.7164563284631023\n",
      "223 0.6813251392901443\n",
      "224 0.6479526331284\n",
      "225 0.6162511402786586\n",
      "226 0.5861419291297847\n",
      "227 0.557545817665751\n",
      "228 0.530372390528889\n",
      "229 0.5045630342255022\n",
      "230 0.48004650357874373\n",
      "231 0.4567428290791705\n",
      "232 0.43459724012302603\n",
      "233 0.4135541816708374\n",
      "234 0.3935676112420235\n",
      "235 0.3745689373196325\n",
      "236 0.35650003762420696\n",
      "237 0.3393315107251235\n",
      "238 0.3230084745773457\n",
      "239 0.30749001847786017\n",
      "240 0.29273854011564515\n",
      "241 0.2787139097833773\n",
      "242 0.2653743553983251\n",
      "243 0.25268854041898303\n",
      "244 0.24062848496681594\n",
      "245 0.22916002865449692\n",
      "246 0.2182502120578691\n",
      "247 0.2078712517638415\n",
      "248 0.19800187678066491\n",
      "249 0.18861015615961213\n",
      "250 0.17967604288671415\n",
      "251 0.17117842008059314\n",
      "252 0.1630930152870961\n",
      "253 0.15539674054873592\n",
      "254 0.14807360073384246\n",
      "255 0.14110630387433037\n",
      "256 0.13447363286937453\n",
      "257 0.1281605579825776\n",
      "258 0.12215360099502345\n",
      "259 0.11643351365975577\n",
      "260 0.11098881299714755\n",
      "261 0.10580462867031987\n",
      "262 0.10087062723076584\n",
      "263 0.09617133439420997\n",
      "264 0.09169740988559913\n",
      "265 0.08743697855370447\n",
      "266 0.08337945734229193\n",
      "267 0.07951401292982699\n",
      "268 0.07583286807791353\n",
      "269 0.07232780756032045\n",
      "270 0.06898864254398632\n",
      "271 0.06580736020432595\n",
      "272 0.06277820696535241\n",
      "273 0.05989020391171217\n",
      "274 0.05713799638320152\n",
      "275 0.0545165089234418\n",
      "276 0.05201885469286688\n",
      "277 0.04963767736445127\n",
      "278 0.04736828091897386\n",
      "279 0.045205570323620214\n",
      "280 0.043144054459256384\n",
      "281 0.04117874206588762\n",
      "282 0.03930589596246112\n",
      "283 0.037520123352767096\n",
      "284 0.03581728673280466\n",
      "285 0.034193942606034436\n",
      "286 0.03264600177071283\n",
      "287 0.031169741559046803\n",
      "288 0.029762404177127697\n",
      "289 0.02842015986365852\n",
      "290 0.027140029977099935\n",
      "291 0.025918674652202156\n",
      "292 0.024753796279220024\n",
      "293 0.023642457243442314\n",
      "294 0.022582563323975815\n",
      "295 0.021571452026007958\n",
      "296 0.020606564793899905\n",
      "297 0.019686022394320567\n",
      "298 0.018807576293416138\n",
      "299 0.01796921274875992\n",
      "300 0.017169281822452045\n",
      "301 0.016405894675931745\n",
      "302 0.01567735197721676\n",
      "303 0.014981759150160439\n",
      "304 0.014318022565692392\n",
      "305 0.013684211317137814\n",
      "306 0.013079161756189856\n",
      "307 0.01250197158550108\n",
      "308 0.01195073128709157\n",
      "309 0.011424308717410271\n",
      "310 0.010921435912671206\n",
      "311 0.010441355988818526\n",
      "312 0.009982832350083523\n",
      "313 0.009544948094683455\n",
      "314 0.009126742966044005\n",
      "315 0.008727483810929509\n",
      "316 0.008345909364172314\n",
      "317 0.007981447679593148\n",
      "318 0.007633345088397421\n",
      "319 0.007300722611690297\n",
      "320 0.006982904484095278\n",
      "321 0.006679338930403534\n",
      "322 0.006389368699093415\n",
      "323 0.006112158330835865\n",
      "324 0.005847238037545862\n",
      "325 0.005594076140117684\n",
      "326 0.005352168455702116\n",
      "327 0.005120927430602785\n",
      "328 0.004899954249868853\n",
      "329 0.004688757727329003\n",
      "330 0.004486834909122125\n",
      "331 0.00429380171094737\n",
      "332 0.004109251797473584\n",
      "333 0.003932825963544052\n",
      "334 0.00376413907101396\n",
      "335 0.003602903145363511\n",
      "336 0.003448695945452509\n",
      "337 0.0033012617332461684\n",
      "338 0.0031602228366971754\n",
      "339 0.0030253511451616847\n",
      "340 0.0028964814658810987\n",
      "341 0.002773106953408874\n",
      "342 0.0026551358494992213\n",
      "343 0.0025422674093203953\n",
      "344 0.0024343265221980406\n",
      "345 0.002331036799529139\n",
      "346 0.002232223879330059\n",
      "347 0.0021376942034264258\n",
      "348 0.002047261079557862\n",
      "349 0.001960744799252786\n",
      "350 0.0018779541759337156\n",
      "351 0.0017987318837140204\n",
      "352 0.0017229223923151348\n",
      "353 0.0016503624705139944\n",
      "354 0.0015809143504398764\n",
      "355 0.0015144666586450256\n",
      "356 0.001450875476895634\n",
      "357 0.0013900035523341106\n",
      "358 0.0013317293506682573\n",
      "359 0.0012759538336304034\n",
      "360 0.0012225581044217199\n",
      "361 0.0011714328504648058\n",
      "362 0.0011224916827337597\n",
      "363 0.0010756431764215942\n",
      "364 0.001030796907781511\n",
      "365 0.0009878412465985308\n",
      "366 0.0009467189638780393\n",
      "367 0.0009073370924366452\n",
      "368 0.0008696232284622798\n",
      "369 0.0008335023897565112\n",
      "370 0.0007989188718640369\n",
      "371 0.0007658143771150554\n",
      "372 0.0007341043772373652\n",
      "373 0.0007037155724722311\n",
      "374 0.0006746105710033572\n",
      "375 0.000646727037171204\n",
      "376 0.0006200167312368264\n",
      "377 0.0005944303803578275\n",
      "378 0.0005699229421744219\n",
      "379 0.0005464462381958127\n",
      "380 0.0005239480718272941\n",
      "381 0.0005023955186879248\n",
      "382 0.0004817432452140158\n",
      "383 0.0004619539631496533\n",
      "384 0.0004429911604199385\n",
      "385 0.0004248211118066485\n",
      "386 0.0004074136815159585\n",
      "387 0.00039072846973337586\n",
      "388 0.0003747369380417548\n",
      "389 0.00035941345715002237\n",
      "390 0.00034472362541722995\n",
      "391 0.0003306444998995309\n",
      "392 0.0003171488980653256\n",
      "393 0.00030421605516039475\n",
      "394 0.0002918212376892626\n",
      "395 0.0002799353376525131\n",
      "396 0.0002685427257662444\n",
      "397 0.0002576212641087967\n",
      "398 0.0002471496654797796\n",
      "399 0.0002371097719071931\n",
      "400 0.00022748547545796307\n",
      "401 0.00021825877077941356\n",
      "402 0.00020941969153310603\n",
      "403 0.00020093574887065398\n",
      "404 0.00019280172535449672\n",
      "405 0.00018500060830593362\n",
      "406 0.0001775194916439579\n",
      "407 0.00017034475710914362\n",
      "408 0.00016346609116496118\n",
      "409 0.00015686824130524136\n",
      "410 0.00015054215704161364\n",
      "411 0.0001444735596658323\n",
      "412 0.0001386535893175986\n",
      "413 0.00013307053296648792\n",
      "414 0.00012771509756503952\n",
      "415 0.0001225785855013755\n",
      "416 0.00011765169027262387\n",
      "417 0.00011292526899144374\n",
      "418 0.0001083928842283544\n",
      "419 0.00010404396104519531\n",
      "420 9.98716574962543e-05\n",
      "421 9.586860947814554e-05\n",
      "422 9.202792656106773e-05\n",
      "423 8.834285095825452e-05\n",
      "424 8.48081460604482e-05\n",
      "425 8.141603169024237e-05\n",
      "426 7.816236963205321e-05\n",
      "427 7.503984082709675e-05\n",
      "428 7.204378268456049e-05\n",
      "429 6.916826288370256e-05\n",
      "430 6.640882582271677e-05\n",
      "431 6.376274470123602e-05\n",
      "432 6.122244342568509e-05\n",
      "433 5.878374754896817e-05\n",
      "434 5.644394803925078e-05\n",
      "435 5.419849003249265e-05\n",
      "436 5.204311694244121e-05\n",
      "437 4.9974123444260774e-05\n",
      "438 4.798843932087817e-05\n",
      "439 4.608235869229052e-05\n",
      "440 4.4253373204771365e-05\n",
      "441 4.249731372217166e-05\n",
      "442 4.081187966727814e-05\n",
      "443 3.919470931549296e-05\n",
      "444 3.764185756282781e-05\n",
      "445 3.6150928112793865e-05\n",
      "446 3.471967447152876e-05\n",
      "447 3.3345718580513e-05\n",
      "448 3.202690918441393e-05\n",
      "449 3.076065112751342e-05\n",
      "450 2.9545106762912082e-05\n",
      "451 2.8378107631753673e-05\n",
      "452 2.7258041502951746e-05\n",
      "453 2.618207271855524e-05\n",
      "454 2.514905740928641e-05\n",
      "455 2.4157250029128904e-05\n",
      "456 2.3205067006345126e-05\n",
      "457 2.229066109278609e-05\n",
      "458 2.1412710634132422e-05\n",
      "459 2.0570023770849087e-05\n",
      "460 1.976099328654747e-05\n",
      "461 1.898364274455841e-05\n",
      "462 1.8237019462113987e-05\n",
      "463 1.752004199230147e-05\n",
      "464 1.6831671851596108e-05\n",
      "465 1.6170435833982066e-05\n",
      "466 1.5535482072140856e-05\n",
      "467 1.4925672951838257e-05\n",
      "468 1.4340049606191664e-05\n",
      "469 1.3777712684008304e-05\n",
      "470 1.3237521021226551e-05\n",
      "471 1.2718620629047167e-05\n",
      "472 1.2220333624265131e-05\n",
      "473 1.1741661641566942e-05\n",
      "474 1.1281927280688794e-05\n",
      "475 1.0840347701199465e-05\n",
      "476 1.041620218333356e-05\n",
      "477 1.000878745379924e-05\n",
      "478 9.617468835210756e-06\n",
      "479 9.241573462349738e-06\n",
      "480 8.880481437294585e-06\n",
      "481 8.53357969815125e-06\n",
      "482 8.20035618781701e-06\n",
      "483 7.880236005616978e-06\n",
      "484 7.572695737342461e-06\n",
      "485 7.277287041952774e-06\n",
      "486 6.9934754533733855e-06\n",
      "487 6.720983813811958e-06\n",
      "488 6.459096386468482e-06\n",
      "489 6.207421053008894e-06\n",
      "490 5.965615868108258e-06\n",
      "491 5.733315736557688e-06\n",
      "492 5.510096129952683e-06\n",
      "493 5.295674891608636e-06\n",
      "494 5.089610051434485e-06\n",
      "495 4.891630469751145e-06\n",
      "496 4.701407202808302e-06\n",
      "497 4.51869708760187e-06\n",
      "498 4.343094550308332e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 4.174361967308919e-06\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## warmup:PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 301.7300720214844\n",
      "199 0.5452671647071838\n",
      "299 0.0017044495325535536\n",
      "399 6.971536640776321e-05\n",
      "499 1.8546084902482107e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTOGRAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 609.012939453125\n",
      "199 3.393815040588379\n",
      "299 0.028269469738006592\n",
      "399 0.0005146132316440344\n",
      "499 7.23170378478244e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 関数化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    torch.autograd.Functionを継承することで自分であつらえたautograd関数を実装することができます。\n",
    "    ここでは順伝播と逆伝播を実装しました。\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        順伝播では、入力を含むテンソルを受け取り、出力を含むテンソルを返しあmす。\n",
    "        ctxはcontextオブジェクトで、逆伝播のための情報をこっそりとっておくことができます。\n",
    "        ctx.save_for_backwardメソッドを使うことで、任意のオブジェクトのキャッシュを逆伝播のためにとっておくことができます。\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        逆伝播では、出力に対するloss関数の勾配を含むテンソルを受け取り、\n",
    "        入力に対するloss関数の勾配を計算する必要がある。\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n",
    "    relu = MyReLU.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # ReLU using our custom autograd operation.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
